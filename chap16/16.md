## Deisgn greedy algorithms:
1. Cast the optimization problem as one in which we make a choice and are left with one subproblem to solve.
2. Prove Greedy choice is always safe.
3. Demonstrate optimal substructure.

## Greedy analysis strategies ⭐⭐⭐
1. Greedy algorithm stays ahead. Show that after each step of the greedy algorithm, its solution is at least as good as any other algorithm's. 
2. Structural. Discover a simple "structural" bound asserting that every possible solution must have a certain value. Then show that your algorithm always achieves this bound.
(Coin Changing)
3. Exchange argument. Gradually transform any solution to the one found by the greedy algorithm without hurting its quality.

### 16.2-1 fractional knapsack greedy-choice property
Sort $n$ items by value per pound $d_i = v_i/w_i$ in decreasing order into $a_1, a_2, ..., a_n$. Greedy choice always pick item with greatest $d_i$. To prove greedy choice is safe, consider subproblem $A_k$: picking most valuable load from remaining items $a_k,a_{k+1},...,a_n$ weighing at most W. Then $a_k$ must be included in some most valuable load if $W>0$.
**Proof:** If $W=0$, no choice can be made. Otherwise let $S_k$ be the most valuable load from remaining items $a_k,a_{k+1},...,a_n$ weighing at most W. First item with greatest value per pound in $S_k$ is $a_j$. If $j>k$, $a_j.value\le a_k.value$. Replace $\min(w_j,w_k)$ pounds of $a_j$ with item $a_k$ we get load $S'_k$ with the same weight with $S_k$ and at least the same total value. Then $a_k$ must be included in some most valuable load if $W>0$.

### 16.2-2 0-1 Knapsack
Let $P_{i,j}$ be subproblem of achieving maximum value out of items $a_i,...,a_n$ weighing at most $j$. Let $c_{i,j}$ be maximum load of value out of remaining items $a_i,...,a_n$ weighing at most $j$.

$$
c_{i,j} = \max\{c_{i+1,j},c_{i+1,j-w_i}+v_i\}
\\
c_{n+1,j}=0,c_{i,j|j<0}=0
$$

```C++
knapSack01(i,j):
    if j<0 or i>n:
        return 0
    if memo.has_value(i,j):
        return memo[i,j]
    choose_i = knapSack01(i+1,j)
    not_choose_i = knapSack01(i+1,j-w[i]) + v[i]
    return max(choose_i, not_choose_i)

```

### 16.2-3
Sort items by increasing weight into $a_1,...,a_n$. Add items from left to right until couldn't fit in.
**Proof:**
Let Greedy solution be $a_1,...,a_j$.
Assume optimal solution $a_i,...,a_m$. If $m=j$, greedy solution is optimal solution. Otherwise $m>j$. $m-i+1 \le j$. $\sum_{k=i}^ma_k\le \sum_{k=1}^ja_k$. Therefore greedy solution is at least as good as optimal solution.

### 16.2-4
Always skate to the farrest station within $m$ miles.
**Prove:**
If any inter-station distance is bigger than $m$, algo returns false.
Let $P_i$ be the problem of skate from station $i$ to $n$ with minimum stops. After we made an greedy choice, we are left with an subproblem $P_j,j>i$. minimum stops starting from station $i$ clearly includes minimum stops starting from station $j$. 
To prove greedy-choice property, notice professor always leave a station with full two liters water. minimum stops starting from later stations must be smaller than or equal to from earlier stations. 
If an optimal solution choose closer next station than greedy choice, substitutes with greedy choice get at least as good solution.

### 16.2-5 ⭐
Always place interval at next point uncovered.
**Prove:**
After each greedy step, $j$ intervals cover maximum possible points. Therefore it costs minimum number of intervals to cover all points.

### 16.2-6 fractional knapsack in $\text{O}(n)$
Similar to finding $k$-th smallest. Partition knapsacks into two halves by value per pound in linear time. Sum up all left weights to $W$. If $W=capacity$, pick all items in the left and return. Else if $W>capacity$, recurse into left half, otherwise recurse into right half. Recurse until no items left, pick all items globally to the left and return.

### 16.2-7 ⭐ [By swapping inversions]
Given two sets A and B, each containing n positive integers. Reorder $A$ and $B$ such that $\prod_{i=1}^n{a_i^{b_i}}$ is maximum.

Maximize $\prod_{i=1}^n{a_i^{b_i}}$ is to maximize $\sum_{i=1}^n{{b_i}\log{a_i}}$. 
Maximum is achieved when $\forall{i},a_i$ and $b_i$ has the same order statistic in each set.

**Prove:**
Without loss of generality, assume $a_1\le ,...,\le a_n$. Let $b_1,...,b_n$ be optimal ordering and $M=\sum_{i=1}^n{{b_i}\log{a_i}}$. If $b_1,...,b_n$ is not in non-decreasing order, there must be inversions in $B$. That is, $\exists i<j, b_i>b_j$. If we were to swap $b_i,b_j$, then get new sum $M'$,

$$
M'-M
\\
= {b_i'}\log{a_i} + {b_j'}\log{a_j} - {b_i}\log{a_i} + {b_j}\log{a_j}
\\
={b_j}\log{a_i} + {b_i}\log{a_j} - {b_i}\log{a_i} - {b_j}\log{a_j}
\\
=(b_j-b_i)(\log{a_i}-\log{a_j})
\\
\ge0
$$

Therefore new ordering is at least as good as optimal ordering of $B$. We know in limited number of swapping inversions we can transform $b_1,...,b_n$ into $b_1'\le,...,\le b_n'$ (simple insertion sort would do) without $M$ decreasing. Non-decreasing ordering of $B$ is optimal.

### 16.3-2
Assume an optimal prefix code tree $T$ has internal node $x$ which links to only one child $y$ by code bit $i$. We can always remove bit $i$ and node $y$ and get new tree $T'$. Every leaf character node $c$ below $y$ has depth $d_{T'}(c)=d_{T}(c)-1$. Total cost $B(T')<B(T)$. Conflicts with assumption $T$ is optimal. Therefore optimal prefix code tree must be a full binary tree.

### 16.3-3
We can prove by induction:

$$
\sum_{k=1}^{i}a_k<a_{i+2}.
$$

Therefore there will only be one tree combined in the huffman process. And it always choose smallest $a_i$ and that tree to construct a new binary tree.

Resulting huffman code:

a:00000001
b:0000001
c:000001
d:00001
e:0001
f:001
g:01
h:1

### 16.3-4
$$
B(T)=\sum_{c\in C}c.freq\cdot d_T(c)
$$

Each $c.freq$ is added $d_T(c)$ times into final sum $B(T)$, as is $c.freq$ added into $d_T(c)$ of $c$'s ancestors. Therefore combining all internal's frequency is $B(T)$

### 16.3-5
If by huffman algorithm, character $c_i$ is chosen later than $c_j$, $d_T(c_i)\le d_T(c_j)$. In $C$, $c_1.freq > c_2.freq>,...,>c_n.freq$. $c_j$ is chosen by huffman before $c_i$ ($i<j$). Then $d_T(c_1)\le,..., d_T(c_n)$.